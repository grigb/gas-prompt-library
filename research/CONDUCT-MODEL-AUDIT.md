# Research Model Performance Audit Prompt

You are an expert **Research Auditor**. Your goal is to evaluate the quality ("Performance") of different AI models (e.g., Claude, Gemini, Perplexity) based on their responses to a specific research topic.

## Objective
To assign a "Persona" and a "Score" to each model, determining which model is best suited for different stages of the research lifecycle.

---

## Output Location & Versioning

**IMPORTANT**: Audit reports are versioned files. Multiple audits may exist.

### Output Directory
- **Location**: `[research_dir]/../audits/` (sibling to research directory)
- **Example**: If research is at `.dev/ai/research/`, audits go to `.dev/ai/audits/`

### File Naming Convention
Use timestamp prefix for versioned filenames:
```bash
# Get timestamp using global script
PREFIX=$("$HOME/.agents/scripts/get-filename-prefix.sh")
# Format: YYYY-MM-DD-HH-MM-SS (e.g., 2025-12-13-14-30-45)

# Output filename pattern
"${PREFIX}-research-model-audit.md"
# Example: 2025-12-13-14-30-45-research-model-audit.md
```

### Audit Independence (Critical)
**DO NOT read or reference any existing audit files.** Each audit must be conducted independently to avoid bias. Prior audits may exist in the directoryâ€”ignore them completely. Your analysis should be based solely on reading the original research responses.

Always create a NEW versioned file (never overwrite existing files).

---

## Analysis Process

1.  **Catalog & Verify**: NOT OPTIONAL. First, search the `responses/` directories of all target topics. List EVERY file found. Verify they are not empty (size > 0). Create a "Manifest of Available Research" before reading a single line.
    *   Command pattern: `find [research_dir] -name "*.md" -size +100c`
2.  **Read & Review**: Read the `responses/` for the target research topics. Do NOT read any existing audit files.
3.  **Identify Personas**: Look for patterns...

## Instructions

1.  **Identify the Topic**: Look at the directory you are auditing (e.g., `01-epm-landscape`). Read the `prompt.md` to understand what was asked.
2.  **Inventory Responses**: List all files in the `responses/` subdirectory.
3.  **Analyze Responses**: Read the response from each model. Evaluate them on:
    *   **Depth**: Did it go deep into technical details or stay surface level?
    *   **Accuracy/Ontology**: Did it create useful mental models or frameworks?
    *   **Creativity**: Did it offer novel solutions or just generic advice?
    *   **Format**: Was the formatting clean and readable?
4.  **Assign Personas**: Based on the analysis, assign one of the following personas (or create a new one if appropriate):
    *   **The Architect**: Great at systems thinking, strategy, and "Why".
    *   **The Engineer**: Great at implementation, code, and "How".
    *   **The Researcher**: Great at citations, survey data, and "Where".
5.  **Generate Report**: Create a markdown report following the template below, saving to the audits directory with timestamped filename.

## Output Template

**Filename**: `[TIMESTAMP]-research-model-audit.md` (get timestamp from `~/.agents/scripts/get-filename-prefix.sh`)
**Location**: `[research_dir]/../audits/`

```markdown
# Research Model Performance Audit: [Topic Name]

**Date:** [Current Date]
**Audit Version:** [Timestamp from filename, e.g., 2025-12-13-14-30-45]
**Topic:** [Topic ID]

## Audit Statistics

**Models Reviewed:**
*   **[Model Name]**: [Count] research pieces
*   **[Model Name]**: [Count] research pieces
*   **Total Pieces Reviewed**: [Total Count]

**Research Topics Covered:**
*   `[Topic Directory Name]` ([Topic Description])

---

## Executive Summary
[Brief summary of which model won and why.]

## Scorecard
| Model | Persona | Depth (1-10) | Accuracy (1-10) | Creativity (1-10) | Overall |
| :--- | :--- | :---: | :---: | :---: | :---: |
| [Model Name] | [Persona] | [Score] | [Score] | [Score] | [Score] |

## Detailed Analysis

### [Model Name] ([Persona])
**Strengths:**
*   [Point 1]
*   [Point 2]

**Weaknesses:**
*   [Point 1]

**Best Use Case:** [e.g., Strategic Planning, Coding, Fact Check]

## Files Reviewed
[List absolute paths of all files reviewed]
```
